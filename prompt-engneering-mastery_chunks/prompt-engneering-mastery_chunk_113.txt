oftware engineer go broke? \n\nBecause he kept fo
his expenses!''' ),
AIMessage (content='''Sure, here \'s a lighthearted joksoftware engineers prefer dark mode? \n\nBecause it \'s
"byte" vision!''' )]
The benefit of using .batch()  over .invoke()  is that you can
parallelize the number of API requests made to OpenAI.
For any runnable in LangChain, you can add a RunnableConfig
argument to the batch  function that contains many configurable
parameters, including max_concurrency :
from langchain_core.runnables.config  import RunnableC
# Create a RunnableConfig with the desired concurrenc
config = RunnableConfig (max_concurrency =5)
# Call the .batch() method with the inputs and config
results = chat.batch([messages , messages ], config=conNOTE
In computer science, asynchr onous (async) functions  are those that operate independently of other
processes, thereby enabling several API requests to be run concurrently without waiting for each
other . In LangChain, these async functions let you make many API requests all at once, not one after
the other . This is especially helpful in more complex workflows and decreases the overall latency to
your users.
Most of the asynchronous functions within LangChain are simply prefixed with the letter a, such as
.ainvoke()  and .abatch() . If you would like to use the async API for more ef ficient task
performance, then utilize these functions.
LangChain Prompt Templates
Up until this point, youâ€™ve been hardcoding the strings in the
ChatOpenAI  objects. As your LLM applications grow in size, it becomes
increasingly important to utilize prompt templates .
Prompt templates  are good for generating reproducible prompts for AI
language models. They consist of a template , a text string that can take in
parameters, and construct a text prompt for a language model.
Without prompt templates, you would likely use Python f-string
formatting:
language  = "Python"
prompt = f"What is the best way to learn coding in {lprint(prompt) # What is the b