 6  but are still not widely used in production at the time of writing.
This practice of self-reasoning agents is still early and prone to errors, but
there are promising signs this approach can be useful in achieving complex
tasks, and is likely to be part of the next stage in evolution for AI systems.
There is an AI battle occurring between lar ge tech firms like Microsoft and
Google, as well as a wide array of open source projects on Hugging Face,
and venture-funded start-ups like OpenAI and Anthropic. As new models
continue to proliferate, they’re diversifying in order to compete for dif ferent
segments of the growing market. For example, Anthropic’ s Claude 2 had an
100,000-token context window , compared to GPT -4’s standard 8,192tokens . OpenAI soon responded with a 128,000-token window version of
GPT-4, and Google touts a 1 million token context length with Gemini 1.5 .
For comparison, one of the Harry Potter books would be around 185,000
tokens, so it may become common for an entire book to fit inside a single
prompt, though processing millions of tokens with each API call may be
cost prohibitive for most use cases.
This book focuses on GPT -4 for text generation techniques, as well as
Midjourney v6 and Stable Dif fusion XL  for image generation techniques,
but within months these models may no longer be state of the art. This
means it will become increasingly important to be able to select the right
model for the job and chain multiple AI systems together . Prompt templates
are rarely comparable when transferring to a new model, but the ef fect of
the Five Prompting Principles will consistently improve any prompt you
use, for any model, getting you more reliable results.
Summary
In this chapter , you learned about the importance of prompt engineering in
the context of generative AI. We defined prompt engineering as the process
of developing ef fective prompts that yield desired results when interacting
with AI models. You discovered that providing clear di