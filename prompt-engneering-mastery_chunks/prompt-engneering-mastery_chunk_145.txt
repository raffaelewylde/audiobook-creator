ad_and_split(text_splitter: TextSplitter |
None = None) -> List[Document]
Simply create a TokenTextSplitter  with a chunk_size=500  and a
chunk_overlap  of 50:from langchain.text_splitter  import TokenTextSplitter
from langchain_community.document_loaders  import PyPD
text_splitter  = TokenTextSplitter (chunk_size =500, chu
loader = PyPDFLoader ("data/principles_of_marketing_bo
pages = loader.load_and_split (text_splitter =text_spli
print(len(pages)) #737
The Principles of Marketing  book contains 497 pages, but after using a
TokenTextSplitter  with a chunk_size  of 500 tokens, you’ve
created 776 smaller LangChain Document  objects.
Text Splitting with Recursive Character
Splitting
Dealing with sizable blocks of text can present unique challenges in text
analysis. A helpful strategy for such situations involves the use of recursive
character splitting . This method facilitates the division of a lar ge body of
text into manageable segments, making further analysis more accessible.
This approach becomes incredibly ef fective when handling generic text. It
leverages a list of characters as parameters and sequentially splits the text
based on these characters. The resulting sections continue to be divideduntil they reach an acceptable size. By default, the character list comprises
"\n\n" , "\n" , " " , and "". This arrangement aims to retain the
integrity of paragraphs, sentences, and words, preserving the semantic
context.
The process hinges on the character list provided and sizes the resulting
sections based on the character count.
Before diving into the code, it’ s essential to understand what the
RecursiveCharacterTextSplitter  does. It takes a text and a list of
delimiters (characters that define the boundaries for splitting the text).
Starting from the first delimiter in the list, the splitter attempts to divide the
text. If the resulting chunks are still too lar ge, it proceeds to the next
delimiter , and so on. This process continues recursively  until the chun