ne_result }
        )
        previous_scene_summary  = summary_result
First, you’ll establish a character_script_generation_chain  in
your script, utilizing various runnables like RunnablePassthrough  for
smooth data flow . Crucially , this chain integrates model =
ChatOpenAI(model='gpt-3.5-turbo-16k') , a powerful model with
a generous 16k context window , ideal for extensive content generation
tasks. When invoked, this chain adeptly generates character scripts, drawing
on inputs such as character profiles, genre, and scene specifics.
You dynamically enrich each scene by adding the summary of the previous
scene, creating a simple yet ef fective buf fer memory . This technique
ensures continuity and context in the narrative, enhancing the LLM’ s ability
to generate coherent character scripts.
Additionally , you’ll see how the StrOutputParser  elegantly converts
model outputs into structured strings, making the generated content easily
usable.DIVIDE LABOR
Remember , designing your tasks in a sequential chain greatly benefits from the Divide Labor
principle. Breaking tasks down into smaller , manageable chains can increase the overall quality of
your output. Each chain in the sequential chain contributes its individual ef fort toward achieving the
overarching task goal.
Using chains gives you the ability to use dif ferent models. For example, using a smart model for the
ideation and a cheap model for the generation usually gives optimal results. This also means you can
have fine-tuned models on each step.
Structuring LCEL Chains
In LCEL  you must ensure that the first part of your LCEL  chain is a
runnable  type. The following code will throw an error:
from langchain_core.prompts.chat  import ChatPromptTem
from operator  import itemgetter
from langchain_core.runnables  import RunnablePassthro
bad_first_input  = {
    "film_required_age" : 18,
}
prompt = ChatPromptTemplate .from_template (
    "Generate a film title, the age is {film_required
)# This will error:
bad_c