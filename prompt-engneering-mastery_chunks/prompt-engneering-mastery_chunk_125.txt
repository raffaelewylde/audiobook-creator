core ['score']
    transaction_category_score ['score']
accuracy_score  = accuracy_score  / (len(transaction_ty
    * 2)
print(f"Accuracy score: {accuracy_score }")
evaluator .evaluate_string_pairs (
    prediction =gpt3pt5_data ,
    prediction_b =mistral_data ,
    input=input_prompt .format(
        format_instructions =output_parser .get_format_
        transaction =transaction ),
    reference =reference_data ,
)
Output:
{'reasoning': '''Both Assistant A and Assistant B pro
response to the user\'s question. Their responses are
correct, and demonstrate depth of thought. They both 
transaction type as "Deposit" and the transaction catthe transaction text provided by the user. Both respo
well-formatted according to the JSON schema provided 
it\'s a tie between the two assistants. \n\nFinal Ver
 'value': None,
 'score': 0.5}
This code demonstrates the simple exact string matching evaluator from
LangChain:
1. evaluator =
load_evaluator("labeled_pairwise_string") : This is a
helper function that can be used to load any LangChain evaluator by
name. In this case, it is the labeled_pairwise_string  evaluator
being used.
2. row = df.iloc[0] : This line and the seven lines that follow get the
first row and extract the values for the dif ferent columns needed. It
includes the transaction description, as well as the Mistral and GPT -3.5
transaction category and types. This is showcasing a single transaction,
but this code can easily run in a loop through each transaction, replacing
this line with an iterrows  function for i, row in
tqdm(df.iterrows(), total=len(df)): , as is done later in the
notebook .
3. gpt3pt5_data = f"""{{ : To use the pairwise comparison
evaluator , we need to pass the results in a way that is formatted correctlyfor the prompt. This is done for Mistral and GPT -3.5, as well as the
reference data.
4. input_prompt = """You are an expert... : The other
formatting we have to get right is in the prompt. To get accurate
evaluation scores, the evaluator need