t correctly labels given text, using
another AI model or rules-based labeling.
Reasoning
Work out which instances the AI fails to apply logical reasoning or
gets the math wrong versus reference cases.
Hallucinations
See how frequently you encouner hallucinations, as measured by
invention of new terms not included in the prompt’ s context.
SafetyFlag any scenarios where the system might return unsafe or
undesirable results using a safety filter or detection system.
Refusals
Find out how often the system incorrectly refuses to fulfill a
reasonable user request by flagging known refusal language.
Adversarial
Make the prompt robust against known prompt injection  attacks that
can get the model to run undesirable prompts instead of what you
programmed.
Similarity
Use shared words and phrases ( BLEU or ROGUE ) or vector distance
(explained in Chapter 5 ) to measure similarity between generated
and reference text.
Once you start rating which examples were good, you can more easily
update the examples used in your prompt as a way to continuously make
your system smarter over time. The data from this feedback can also feed
into examples for fine-tuning, which starts to beat prompt engineering once
you can supply a few thousand examples , as shown in Figure 1-13 .Figure 1-13. How many data points is a prompt worth?Graduating from thumbs-up or thumbs-down, you can implement a 3-, 5-,
or 10-point rating system to get more fine-grained feedback on the quality
of your prompts. It’ s also possible to determine aggregate relative
performance through comparing responses side by side, rather than looking
at responses one at a time. From this you can construct a fair across-model
comparison using an Elo rating , as is popular in chess and used in the
Chatbot Arena  by lmsys.or g.
For image generation, evaluation usually takes the form of permutation
prompting, where you input multiple directions or formats and generate an
image for each combination. Images can than be scanned or later