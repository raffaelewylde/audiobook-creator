en library based on token usage:
    get_text_length =num_tokens_from_string ,
)
First, you set up a PromptTemplate  that takes two input variables for
each example. Then LengthBasedExampleSelector  adjusts the
number of examples according to the length of the examples input , ensuring
your LLM doesn’ t generate a story beyond its context window .
Also, you’ve customized the get_text_length  function to use the
num_tokens_from_string  function that counts the total number of
tokens using tiktoken . This means that max_length=1000  represents
the number of tokens  rather than using the following default function:get_text_length: Callable[[str], int] = lambda x:
len(re.split("\n| ", x))
Now , to tie all these elements together:
dynamic_prompt  = FewShotPromptTemplate (
    example_selector =example_selector ,
    example_prompt =story_prompt ,
    prefix='''Generate a story for {character}  using 
    current Character/Story pairs from all of the cha
    as context.''' ,
    suffix="Character: {character} \nStory:",
    input_variables =["character" ],
)
# Provide a new character from Lord of the Rings:
formatted_prompt  = dynamic_prompt .format(character ="F
# Creating the chat model:
chat = ChatOpenAI ()
response  = chat.invoke([SystemMessage (content=formatt
print(response .content)
Output:Frodo was a young hobbit living a peaceful life in th
his life...
PROVIDE EXAMPLES AND SPECIFY FORMAT
When working with few-shot examples, the length of the content matters in determining how many
examples the AI model can take into account. Tune the length of your input content and provide apt
examples for ef ficient results to prevent the LLM from generating content that might surpass its
context window limit.
After formatting the prompt, you create a chat model with
ChatOpenAI()  and load the formatted prompt into a SystemMessage
that creates a small story about Frodo from Lord of the Rings .
Rather than creating and formatting a ChatPromptTemplate , it’s often
much easier to 