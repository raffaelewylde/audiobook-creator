their Python
package with pip install openai .
As discussed in Chapter 1 , it’s best practice to set an environment variable
called OPENAI_API_KEY  in your terminal or load it from an .env file
using python-dotenv . However , for prototyping you can choose to skip
this step by passing in your API key directly when loading a chat model in
LangChain:from langchain_openai.chat_models  import ChatOpenAI
chat = ChatOpenAI (api_key="api_key" )
WARNING
Hardcoding API keys in scripts is not recommended due to security reasons. Instead, utilize
environment variables or configuration files to manage your keys.
In the constantly evolving landscape of LLMs, you can encounter the
challenge of disparities across dif ferent model APIs. The lack of
standardization in interfaces can induce extra layers of complexity in
prompt engineering and obstruct the seamless integration of diverse models
into your projects.
This is where LangChain comes into play . As a comprehensive framework,
LangChain allows you to easily consume the varying interfaces of dif ferent
models.
LangChain’ s functionality ensures that you aren’ t required to reinvent your
prompts or code every time you switch between models. Its platform-
agnostic approach promotes rapid experimentation with a broad range of
models, such as Anthropic , Vertex AI, OpenAI , and BedrockChat . This not
only expedites the model evaluation process but also saves critical time and
resources by simplifying complex model integrations.In the sections that follow , you’ll be using the OpenAI package and their
API in LangChain.
Chat Models
Chat models such as GPT -4 have become the primary way to interface with
OpenAI’ s API. Instead of of fering a straightforward “input text, output text”
response, they propose an interaction method where chat messages  are the
input and output elements.
Generating LLM responses using chat models involves inputting one or
more messages into the chat model. In the context of LangChain, the
currently accepted