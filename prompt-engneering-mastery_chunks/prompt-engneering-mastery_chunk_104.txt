initializes an empty list to store the conciseness
ratings.
9. Another for  loop iterates over each response in responses .
10. For each response, the script sends it along with the system_prompt
to the OpenAI API, requesting a conciseness evaluation. This time, the
model used is “gpt-4.”
11. The evaluation rating (either 1 for concise or 0 for not concise) is then
stripped of whitespace and added to the ratings  list.
12. The final for  loop iterates over the ratings  list. For each rating, it
prints the style  of the response (either “concise” or “verbose”) and its
corresponding conciseness rating .
For simple ratings like conciseness, GPT -4 performs with near 100%
accuracy; however , for more complex ratings, it’ s important to spend some
time evaluating the evaluator . For example, by setting test cases that containan issue, as well as test cases that do not contain an issue, you can identify
the accuracy of your evaluation metric. An evaluator can itself be evaluated
by counting the number of false positives (when the LLM hallucinates an
issue in a test case that is known not to contain an issue), as well as the
number of false negatives (when the LLM misses an issue in a test case that
is known to contain an issue). In our example we generated the concise and
verbose examples, so we can easily check the rating accuracy , but in more
complex examples you may need human evaluators to validate the ratings.
EVALUATE QUALITY
Using GPT -4 to evaluate the responses of less sophisticated models is an emer ging standard practice,
but care must be taken that the results are reliable and consistent.
Compared to human-based evaluation, LLM-based or synthetic evaluation
typically costs an order of magnitude less and completes in a few minutes
rather than taking days or weeks. Even in important or sensitive cases
where a final manual review by a human is necessary , rapid iteration and
A/B testing of the prompt through synthetic reviews can save significant
time and improv