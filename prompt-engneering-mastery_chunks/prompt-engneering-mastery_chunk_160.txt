n these models are trained, texts that appear together in the training
data will be pushed closer together in values, and texts that are unrelated
will be pushed further away . Imagine we trained a simple model with only
two parameters, Cartoon  and Hygiene , that must describe the entire
world, but only in terms of these two variables. Starting from the word
mouse , increasing the value for the parameter Cartoon  we would travel
toward the most famous cartoon mouse, mickey mouse , as shown in
Figure 5-1 . Decreasing the value for the Hygiene  parameter would take
us toward rat , because rats are rodents similar to mice, but are associated
with plague and disease (i.e., being unhygenic).Figure 5-1. 2-D vector distances
Each location on the graph can be found by two numbers on the x- and y-
axes, which represent the features of the model Cartoon  and Hygiene .
In reality , vectors can have thousands of parameters, because having more
parameters allows the model to capture a wider range of similarities and
differences. Hygiene is not the only dif ference between mice and rats, and
Mickey Mouse isn’ t just a cartoon mouse. These features are learned from
the data in a way that makes them hard for humans to interpret, and we
would need a graph with thousands of axes to display a location in latent
space  (the abstract multidimensional space formed by the model’ s
parameters). Often there is no human-understandable explanation of what afeature means. However , we can create a simplified two-dimensional
projection of the distances between vectors, as has been done in Figure 5-2 .
To conduct a vector search, you first get the vector (or location) of what you
want to look up and find the k closest records in the database. In this case
the word mouse  is closest to mickey mouse , cheese , and trap  where
k=3  (return the three nearest records). The word rat is excluded if k=3 ,
but would be included if k=4  as it is the next closest vector . The word
airplane  in this exampl