 You could say... I'm a primary source.
    ---
    SCENE NUMBER: {index}
    """,
)
summarize_prompt  = ChatPromptTemplate .from_template (    template ="""Given a character script, create a su
    Character script: {character_script} """,
)
Technically , you could generate all of the scenes asynchronously . However ,
itâ€™s beneficial to know what each character has done in the previous scene to
avoid r epeating points .
Therefore, you can create two LCEL  chains, one for generating the
character scripts per scene and the other for summarizations of previous
scenes:
# Loading a chat model:
model = ChatOpenAI (model='gpt-3.5-turbo-16k' )
# Create the LCEL chains:
character_script_generation_chain  = (
    {
        "characters" : RunnablePassthrough (),
        "genre": RunnablePassthrough (),
        "previous_scene_summary" : RunnablePassthrough
        "plot": RunnablePassthrough (),
        "scene": RunnablePassthrough (),
        "index": RunnablePassthrough (),
    }
    | character_script_prompt    | model
    | StrOutputParser ()
)
summarize_chain  = summarize_prompt  | model | StrOutpu
# You might want to use tqdm here to track the progre
# or use all of the scenes:
for index, scene in enumerate (scenes[0:3]):
    # # Create a scene generation:
    scene_result  = character_script_generation_chain .
        {
            "characters" : story_result ["characters" ],
            "genre": "fantasy" ,
            "previous_scene_summary" : previous_scene_
            "index": index,
        }
    )
    # Store the generated scenes:
    generated_scenes .append(
        {"character_script" : scene_result , "scene": s
    )
    # If this is the first scene then we don't have a
    # previous scene summary:
    if index == 0:        previous_scene_summary  = scene_result
    else:
        # If this is the second scene or greater then
        # we can use and generate a summary:
        summary_result  = summarize_chain .invoke(
            {"character_script" : sce