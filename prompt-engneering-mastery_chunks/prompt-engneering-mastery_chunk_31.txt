 t have caught
until you started using it in production. The downside is that it can get
tedious rating lots of responses manually , and your ratings might not
represent the preferences of your intended audience. However , even smallnumbers of tests can reveal lar ge dif ferences between two prompting
strategies and reveal nonobvious issues before reaching production.
Iterating on and testing prompts can lead to radical decreases in the length
of the prompt and therefore the cost and latency of your system. If you can
find another prompt that performs equally as well (or better) but uses a
shorter prompt, you can af ford to scale up your operation considerably .
Often you’ll find in this process that many elements of a complex prompt
are completely superfluous, or even counterproductive.
The thumbs-up  or other manually labeled indicators of quality don’ t have to
be the only judging criteria. Human evaluation is generally considered to be
the most accurate form of feedback. However , it can be tedious and costly
to rate many samples manually . In many cases, as in math or classification
use cases, it may be possible to establish ground truth  (reference answers to
test cases) to programmatically rate the results, allowing you to scale up
considerably your testing and monitoring ef forts. The following is not an
exhaustive list because there are many motivations for evaluating your
prompt programmatically:
Cost
Prompts that use a lot of tokens, or work only with more expensive
models, might be impractical for production use.
LatencyEqually the more tokens there are, or the lar ger the model required,
the longer it takes to complete a task, which can harm user
experience.
Calls
Many AI systems require multiple calls in a loop to complete a task,
which can seriously slow down the process.
Performance
Implement some form of external feedback system, for example a
physics engine or other model for predicting real-world results.
Classification
Determine how often a promp