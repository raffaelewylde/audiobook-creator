ks are
small enough or all delimiters are exhausted.
Using the preceding text  variable, start by importing
RecursiveCharacterText Splitter . This instance will be
responsible for splitting the text. When initializing the splitter , parameters
chunk_size , chunk_overlap , and length_function  are set.
Here, chunk_size  is set to 100, and chunk_overlap  to 20.
The length_function  is defined as len  to determine the size of the
chunks. Itâ€™ s also possible to modify the length_function  argument touse a tokenizer count instead of using the default len  function, which will
count characters:
from langchain_text_splitters  import RecursiveCharact
text_splitter  = RecursiveCharacterTextSplitter (
    chunk_size =100,
    chunk_overlap =20,
    length_function =len,
)
Once the text_splitter  instance is ready , you can use .split_text
to split the text  variable into smaller chunks. These chunks are stored in
the texts  Python list:
# Split the text into chunks:
texts = text_splitter .split_text (text)
As well as simply splitting the text with overlap into a list of strings, you
can easily create LangChain Document  objects with the
.create_documents  function. Creating Document  objects is useful
because it allows you to:
Store documents within a vector database for semantic search
Add metadata to specific pieces of textIterate over multiple documents to create a higher -level summary
To add metadata, provide a list of dictionaries to the metadatas
argument:
# Create documents from the chunks:
metadatas  = {"title": "Biology" , "author" : "John Doe"
docs = text_splitter .create_documents (texts, metadata
But what if your existing Document  objects are too long?
You can easily handle that by using the .split_documents  function
with a TextSplitter . This will take in a list of Document  objects and
will return a new list of Document  objects based on your
TextSplitter  class ar gument settings:
text_splitter  = RecursiveCharacterTextSplitter (chunk_
splitted_docs  = text_