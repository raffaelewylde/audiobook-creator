 for i in range(0, len(tex
for chunk in chunks:
    print("-" * 20)
    print(chunk)Output:
search engine optimization strategy for many local bu
Google My Business profile to appear in local search 
products or services related to what yo
--------------------
u offer.
For Keeps Bookstore, a local bookstore in Atlanta, GA
Google My Business profile for local SEO so it appear
“atlanta bookstore.”
--------------------
...(shortened for brevity)...
First, you open the text file hubspot_blog_post.txt  with the open  function
and read its contents into the variable text. Then using a list comprehension
you create a list of chunks, where each chunk  is a 200 character substring
of text.
Then you use the range  function to generate indices for each 200
character substring, and the i:i+200  slice notation to extract the
substring from text.
Finally , you loop through each chunk in the chunks  list and print  it to
the console.As you can see, because the chunking implementation is relatively simple
and only based on length, there are gaps within the sentences and even
words.
For these reasons we believe that good NLP  chunking has the following
properties:
Preserves entire words, ideally sentences and contextual points made by
speakers
Handles for when sentences span across several pages, for example, page
1 into page 2
Provides an adequate token count for each chunk  so that the total
number of input tokens will appropriately fit into a given token context
window for any LLM
Sliding Window Chunking
Sliding window chunking  is a technique used for dividing text data into
overlapping chunks, or windows , based on a specified number of characters,
tokens, or words.
But what exactly is a sliding window?
Imagine viewing a long piece of text through a small window . This window
is only capable of displaying a fixed number of items at a time. As you slidethis window from the beginning to the end of the text, you see overlapping
chunks of text . This mechanism forms the essence of 