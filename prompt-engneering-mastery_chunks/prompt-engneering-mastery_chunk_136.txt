on final_prompt , your few-shot
examples are added after the SystemMessage .
Notice that the LLM only returns 'Washington, D.C.'  This is because
after the LLMs response is returned, it is parsed  by
StrOutputParser() , an output parser . Adding StrOutputParser()
is a common way to ensure that LLM responses in chains return string
values . You’ll explore this more in depth while learning sequential chains in
LCEL.Selecting Few-Shot Examples by Length
Before diving into the code, let’ s outline your task. Imagine you’re building
a storytelling application powered by GPT -4. A user enters a list of
character names with previously generated stories. However , each user ’s list
of characters might have a dif ferent length. Including too many characters
might generate a story that surpasses the LLM’ s context window limit.
That’ s where you can use LengthBasedExampleSelector  to adapt the
prompt according to the length of user input:
from langchain_core.prompts  import FewShotPromptTempl
from langchain.prompts.example_selector  import Length
from langchain_openai.chat_models  import ChatOpenAI
from langchain_core.messages  import SystemMessage
import tiktoken
examples  = [
    {"input": "Gollum" , "output" : "<Story involving G
    {"input": "Gandalf" , "output" : "<Story involving 
    {"input": "Bilbo", "output" : "<Story involving Bi
]
story_prompt  = PromptTemplate (
    input_variables =["input", "output" ],
    template ="Character: {input}\nStory: {output} ",
)def num_tokens_from_string (string: str) -> int:
    """Returns the number of tokens in a text string.
    encoding  = tiktoken .get_encoding ("cl100k_base" )
    num_tokens  = len(encoding .encode(string))
    return num_tokens
example_selector  = LengthBasedExampleSelector (
    examples =examples ,
    example_prompt =story_prompt ,
    max_length =1000, # 1000 tokens are to be included
    # get_text_length: Callable[[str], int] = lambda 
    # You have modified the get_text_length function 
    # TikTok