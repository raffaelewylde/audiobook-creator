ch by vector stores.
Vector databases (vector stor es)These databases can save and execute searches over embedded data.
Retrievers
These tools of fer the capability to query and retrieve data.
Also, it’ s worth mentioning that other LLM frameworks such as
LlamaIndex  work seamlessly with LangChain. LlamaHub  is another open
source library dedicated to document loaders and can create LangChain-
specific Document  objects.
Document Loaders
Let’s imagine you’ve been tasked with building an LLM data collection
pipeline for NutriFusion Foods. The information that you need to gather for
the LLM is contained within:
A PDF of a book called Principles of Marketing
Two .docx  marketing reports in a public Google Cloud Storage bucket
Three .csv files showcasing the marketing performance data for 2021,
2022, and 2023
Create a new Jupyter Notebook or Python file in content/chapter_4  of the
shared repository , and then run pip install pdf2image docx2txt
pypdf , which will install three packages.All of the data apart from .docx  files can be found in
content/chapter_4/data . You can start by importing all of your various data
loaders and creating an empty all_documents  list to store all of the
Document  objects across your data sources.
Input:
from langchain_community.document_loaders  import Docx
from langchain_community.document_loaders  import PyPD
from langchain_community.document_loaders.csv_loader  
import glob
from langchain.text_splitter  import CharacterTextSpli
# To store the documents across all data sources:
all_documents  = []
# Load the PDF:
loader = PyPDFLoader ("data/principles_of_marketing_bo
pages = loader.load_and_split ()
print(pages[0])
# Add extra metadata to each page:
for page in pages:
    page.metadata ["description" ] = "Principles of Mar
# Checking that the metadata has been added:
for page in pages[0:2]:    print(page.metadata )
# Saving the marketing book pages:
all_documents .extend(pages)
csv_files  = glob.glob("data/*.csv" )
# Filter to only incl