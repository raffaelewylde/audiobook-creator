ame is James".
## Instructions
Please answer the user message using the context abov
User message: What is my name?
AI message:
It’s impossible to pass all 3,000 past messages into the prompt for most
models, and for a traditional search the AI model would have to formulate
the right search query , which can be unreliable. Using the RAG pattern, you
would pass the current user message to a vector search function, and return
the most relevant three records as context, which the chatbot can then use to
respond correctly .GIVE DIRECTION
Rather than inserting static knowledge into the prompt, vector search allows you to dynamically
insert the most relevant knowledge into the prompt.
Here’ s how the process works for production applications using RAG:
1. Break documents into chunks of text.
2. Index chunks in a vector database.
3. Search by vector for similar records.
4. Insert records into the prompt as context.
In this instance, the documents would be all the 3,000 past user messages to
serve as the chatbot’ s memory , but it could also be sections of a PDF
document we uploaded to give the chatbot the ability to read, or a list of all
the relevant products you sell to enable the chatbot to make a
recommendation. The ability of our vector search to find the most similar
texts is wholly dependent on the AI model used to generate the vectors,
referred to as embeddings  when you’re dealing with semantic or contextual
information.
Introducing Embeddings
The word embeddings  typically refers to the vector representation of the
text returned from a pretrained AI model. At the time of writing, thestandard model for generating embeddings is OpenAI’ s text-embedding-
ada-002, although embedding models have been available long before the
advent of generative AI.
Although it is helpful to visualize vector spaces as a two-dimensional chart,
as in Figure 5-2 , in reality the embeddings returned from text-embedding-
ada-002 are in 1,536 dimensions, which is dif ficult to depict graph