tracts the first transaction description from a dataframe df. This
dataframe is loaded earlier in the Jupyter Notebook  (omitted for brevity).
10. for i, row in tqdm(df.iterrows(), total=len(df)) :
Iterates over each row in the dataframe df, with a progress bar .
11. result = chain.invoke(...) : Inside the loop, the chain is
invoked for each transaction.
12. except : In case of an exception, a default
EnrichedTransactionInformation  object with None  values is
created. These will be treated as errors in evaluation but will not break
the processing loop.13. df["mistral_transaction_type"] = transaction_types ,
df["mistral_transaction_category"] =
transaction_categories : Adds the transaction types and
categories as new columns in the dataframe, which we then display with
df.head() .
With the responses from Mistral saved in the dataframe, it’ s possible to
compare them to the transaction categories and types defined earlier to
check the accuracy of Mistral. The most basic LangChain eval metric is to
do an exact string match of a prediction against a reference answer , which
returns a score of 1 if correct, and a 0 if incorrect. The notebook gives an
example of how to implement this , which shows that Mistral’ s accuracy is
77.5%. However , if all you are doing is comparing strings, you probably
don’t need to implement it in LangChain.
Where LangChain is valuable is in its standardized and tested approaches to
implementing more advanced evaluators using LLMs. The evaluator
labeled_pairwise_string  compares two outputs and gives a reason
for choosing between them, using GPT -4. One common use case for this
type of evaluator is to compare the outputs from two dif ferent prompts or
models, particularly if the models being tested are less sophisticated than
GPT-4. This evaluator using GPT -4 does still work for evaluating GPT -4
responses, but you should manually review the reasoning and scores to
ensure it is doing a good job: if GPT -4 is bad at a task, it may also be bad