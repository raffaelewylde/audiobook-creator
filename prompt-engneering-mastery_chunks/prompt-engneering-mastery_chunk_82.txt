the sliding window
approach.
Each window size is defined by a fixed number of characters, tokens, or
words, and the step size  determines how far the window moves with each
slide.
In Figure 3-6 , with a window size of 4 words and a step size of 1, the first
chunk would contain the first 4 words of the text. The window then slides 1
word to the right to create the second chunk, which contains words 2
through 5.
This process repeats until the end of the text is reached, ensuring each
chunk overlaps with the previous and next ones to retain some shared
context.Figure 3-6. A sliding window , with a window size of 4 and a step size of 1
Due to the step size being 1, there is a lot of duplicate information between
chunks, and at the same time the risk of losing information between chunks
is dramatically reduced.
This is in stark contrast to Figure 3-7 , which has a window size of 4 words
and a step size of 2. You’ll notice that because of the 100% increase in step
size, the amount of information shared between the chunks is greatly
reduced.Figure 3-7. A sliding window , with a window size of 4 and a step size of 2
You will likely need a lar ger overlap if accuracy and preserving semanatic
context are more important than minimizing token inputs or the number of
requests made to an LLM.
Example 3-5  shows how you can implement a sliding window using
Python’ s len()  function. The len()  function provides us with the total
number of characters rather than words in a given text string, which
subsequently aids in defining the parameters of our sliding windows.
Example 3-5. Sliding window
def sliding_window (text, window_size , step_size ):
    if window_size  > len(text) or step_size  < 1:
        return []    return [text[i:i+window_size ] for i
    in range(0, len(text) - window_size  + 1, step_siz
text = "This is an example of sliding window text chu
window_size  = 20
step_size  = 5
chunks = sliding_window (text, window_size , step_size )
for idx, chunk in enumerate (chunks