nce of LLMs open source and encourages others to contribute
additional eval templates.
In addition to the standard academic evals, there are also more headline-
worthy tests like GPT-4 passing the bar exam . Evaluation is dif ficult for
more subjective tasks, and can be time-consuming or prohibitively costly
for smaller teams. In some instances researchers have turned to using more
advanced models like GPT -4 to evaluate responses from less sophisticated
models, as was done with the release of Vicuna-13B , a fine-tuned model
based on Meta’ s Llama open source model (see Figure 1-1 1).Figure 1-1 1. Vicuna GPT -4 Evals
More rigorous evaluation techniques are necessary when writing scientific
papers or grading a new foundation model release, but often you will only
need to go just one step above basic trial and error . You may find that a
simple thumbs-up/thumbs-down rating system implemented in a Jupyter
Notebook can be enough to add some rigor to prompt optimization, without
adding too much overhead. One common test is to see whether providing
examples is worth the additional cost in terms of prompt length, or whether
you can get away with providing no examples in the prompt. The first step
is getting responses for multiple runs of each prompt and storing them in a
spreadsheet, which we will do after setting up our environment.
You can install the OpenAI Python package with pip install openai .
If you’re running into compatability issues with this package, create a
virtual environment and install our requir ements.txt  (instructions in the
preface).To utilize the API, you’ll need to create an OpenAI account  and then
navigate here for your API key .
WARNING
Hardcoding API keys in scripts is not recommended due to security reasons. Instead, utilize
environment variables or configuration files to manage your keys.
Once you have an API key , it’s crucial to assign it as an environment
variable by executing the following command, replacing api_key  with
your actual API k