 deals with {context} ?
Here is an example of the format:
1. Name1
2. Name2
3. Name3
4. Name4
5. Name5
"""
model = ChatOpenAI ()
system_prompt  = SystemMessagePromptTemplate .from_temp
chat_prompt  = ChatPromptTemplate .from_messages ([syste
chain = chat_prompt  | model
result = chain.invoke({
    "industry" : "medical" ,
    "context" :'''creating AI solutions by automatical
    records''' ,    "principles" :'''1. Each name should be short and 
    remember. 2. Each name should be easy to pronounc
    3. Each name should be unique and not already tak
})
print(result.content)
Output:
1. SummarAI
2. MediSummar
3. AutoDocs
4. RecordAI
5. SmartSummarize
First, you’ll import ChatOpenAI , SystemMessagePromptTemplate ,
and ChatPromptTemplate . Then, you’ll define a prompt template with
specific guidelines under template , instructing the LLM to generate
business names. ChatOpenAI()  initializes the chat, while
SystemMessagePromptTemplate.from_template(template)  and
ChatPromptTemplate.from_messages([system_prompt])  create
your prompt template.
You create an LCEL  chain  by piping together chat_prompt  and the
model , which is then invoked . This replaces the {industries} ,{context} , and {principles}  placeholders in the prompt with the
dictionary values within the invoke  function.
Finally , you extract the LLM’ s response as a string accessing the
.content  property on the result  variable.
GIVE DIRECTION AND SPECIFY FORMAT
Carefully crafted instructions might include things like “Y ou are a creative consultant brainstorming
names for businesses” and “Please generate a numerical list of five to seven catchy names for a start-
up.” Cues like these guide your LLM to perform the exact task you require from it.
Using PromptTemplate with Chat
Models
LangChain provides a more traditional template called PromptTemplate ,
which requires input_variables  and template  arguments.
Input:
from langchain_core.prompts  import PromptTemplate
from langchain.prompts.chat  import System