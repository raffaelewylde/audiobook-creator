M outputs
might not always be in your desired format.
Output parsers save you from the complexity and intricacy of regular
expressions, providing easy-to-use functionalities for a variety of use cases.
Now that you’ve seen them in action, you can utilize output parsers to
effortlessly structure and retrieve the data you need from an LLM’ s output,
harnessing the full potential of AI for your tasks.
Furthermore, using parsers to structure the data extracted from LLMs
allows you to easily choose how to or ganize outputs for more ef ficient use.
This can be useful if you’re dealing with extensive lists and need to sort
them by certain criteria, like business names.
LangChain Evals
As well as output parsers to check for formatting errors, most AI systems
also make use of evals , or evaluation metrics, to measure the performance
of each prompt response. LangChain has a number of of f-the-shelf
evaluators, which can be directly be logged in their LangSmith  platform forfurther debugging, monitoring, and testing. Weights and Biases  is
alternative machine learning platform that of fers similar functionality and
tracing capabilities for LLMs.
Evaluation metrics are useful for more than just prompt testing, as they can
be used to identify positive and negative examples for retrieval as well as to
build datasets for fine-tuning custom models.
Most eval metrics rely on a set of test cases, which are input and output
pairings where you know the correct answer . Often these reference answers
are created or curated manually by a human, but it’ s also common practice
to use a smarter model like GPT -4 to generate the ground truth answers,
which has been done for the following example. Given a list of descriptions
of financial transactions, we used GPT -4 to classify each transaction with a
transaction_category  and transaction_type . The process can
be found in the langchain-evals.ipynb  Jupyter Notebook in the
GitHub repository  for the book.
With the GPT -4 answer being taken as