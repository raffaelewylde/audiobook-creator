ce
in the dataset will have vectors created for them.
5. workers=4 : Number of CPU cores to use during training. It speeds
up training on multicore machines.
6. seed=36 : This is set for reproducibility so that the random
processes in training deliver the same result each time (not guaranteed
with multiple workers).
3. After training, the model is saved to a file named
custom_word2vec_model.model  using the save  method. This
allows you to reuse the trained model later without needing to train it
again.
4. There is a commented-out line that shows how to load the model back
from the saved file. This is useful when you want to load a pre-trained
model in a dif ferent script or session.5. The variable vector  is assigned the vector representation of the word
cake. This vector can be used for various purposes, like similarity
calculations, arithmetic operations, etc.
6. The most_similar  method is used to find words that are most similar
to the provided vector (in this case, the vector for cake). The method
returns the top five ( topn=5 ) most similar words.
7. The similarity  method queries the similarity between cake and lie
direction, showing a small positive value.
The dataset is small and heavily repetitive, which might not provide a
diverse context to properly learn the relationship between the words.
Normally , word2vec benefits from lar ger and more diverse corpora and
typically won’ t get good results until you’re into the tens of millions of
words. In the example we set a seed value to cherrypick one instance where
lie came back in the top five results, but if you remove that seed, you’ll find
it rarely discovers the association successfully .
For smaller document sizes a simpler technique TF-IDF  (Term Frequency-
Inverse Document Frequency) is recommended, a statistical measure used
to evaluate the importance of a word in a document relative to a collection
of documents. The TF-IDF value increases proportionally to the number of
times a word appears in the do