ke", "is", "definitely" , "a", "lie"],
    ["everyone" , "knows", "that", "cake", "equals" , "
    # ...
] * 10  # repeat several times to emphasize
# Train the word2vec model
model =  Word2Vec (sentences , vector_size =100, window=
min_count =1, workers=4, seed=36)
# Save the model
model.save("custom_word2vec_model.model" )
# To load the model later
# loaded_model = word2vec.load(
# "custom_word2vec_model.model")
# Get vector for a word
vector = model.wv['cake']# Find most similar words
similar_words  = model.wv.most_similar ("cake", topn=5)
print("Top five most similar words to 'cake': " , simi
# Directly query the similarity between "cake" and "l
cake_lie_similarity  = model.wv.similarity ("cake", "li
print("Similarity between 'cake' and 'lie': " ,
cake_lie_similarity )
Output:
Top 5 most similar words to 'cake':  [('lie',
0.23420444130897522), ('test', 0.23205122351646423),
('tests', 0.17178669571876526), ('GLaDOS',
0.1536172330379486), ('got', 0.14605288207530975)]
Similarity between 'cake' and 'lie':  0.23420444
This code creates a word2vec model using the Gensim library and then uses
the model to determine words that are similar to a given word. Let’ s break it
down:
1. The variable sentences  contains a list of sentences, where each
sentence is a list of words. This is the data on which the Word2V ec
model will be trained. In a real application, instead of such hardcodedsentences, you’d often load a lar ge corpus of text and preprocess it to
obtain such a list of tokenized sentences.
2. An instance of the word2vec  class is created to represent the model.
While initializing this instance, several parameters are provided:
1. sentences : This is the training data.
2. vector_size=100 : This defines the size of the word vectors. So
each word will be represented as a 100-dimensional vector .
3. window=5 : This represents the maximum distance between the
current and predicted word within a sentence.
4. min_count=1 : This ensures that even words that appear only on