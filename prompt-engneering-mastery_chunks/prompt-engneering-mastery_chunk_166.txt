{"wait_for_model" :True}})
    return response .json()
texts = ["mickey mouse" ,
        "cheese" ,
        "trap",
        "rat",
        "ratatouille"
        "bus",
        "airplane" ,
        "ship"]
output = query(texts)
output
Output:
[[-0.03875632584095001, 0.04480459913611412,
0.016051070764660835, -0.01789097487926483,
-0.03518553078174591, -0.013002964667975903,
0.14877274632453918, 0.048807501792907715,
0.011848390102386475, -0.044042471796274185,
...-0.026688814163208008, -0.0359361357986927,
-0.03237859532237053, 0.008156519383192062,
-0.10299170762300491, 0.0790356695652008,
-0.008071334101259708, 0.11919838190078735,
0.0005506130401045084, -0.03497892618179321]]
This code uses the Hugging Face API to obtain embeddings for a list of text
inputs using a pre-trained model. The model used here is the sentence-
transformers/all-MiniLM-L6-v2 , which is a smaller version of
BER T, an open source NLP  model introduced by Google in 2017 (based on
the transformer model), which is optimized for sentence-level tasks. Hereâ€™ s
how it works step-by-step:
1. model_id  is assigned the identifier of the pre-trained model,
sentence-transformers/all-MiniLM-L6-v2 .
2. hf_token = os.getenv("HF_TOKEN")  retrieves the API key for
the Hugging Face API token from your environment. You need to set this
in your environment with your own token, which can be obtained by
creating an account and visiting https://hf.co/settings/tokens .
3. The requests  library is imported to make HTTP  requests to the API.
4. api_url  is assigned the URL  for the Hugging Face API, with the
model ID included in the URL.
5. headers  is a dictionary containing the authorization header with your
Hugging Face API token.6. The query()  function is defined, which takes a list of text inputs and
sends a POST  request to the Hugging Face API with the appropriate
headers and JSON payload containing the inputs and an option to wait
for the model to become available. The function then returns the JSON
response